---
title: "Quick and dirty analysis of my Twitter social network"
author: "Olivier Gimenez"
date: "7/26/2021"
output: 
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, dpi = 300)
```

## Motivation

I use Twitter to get live updates of what my follow scientists are up to, to communicate about my students' awesome work and to share material that I hope is useful to some people.  
Recently, I reached 5,000 followers and I thought I'd spend some time trying to know better who they/you are. To do so, I use `R` to retrieve some data from Twitter using [`rtweet`](https://docs.ropensci.org/rtweet/), do some data exploration and visualisation using the [`tidyverse`](https://www.tidyverse.org/) and examine my network of followers with [`tidygraph`](https://tidygraph.data-imaginist.com/), [`ggraph`](https://ggraph.data-imaginist.com/) and [`igraph`](https://igraph.org/r/). 

To reproduce the analyses below, you will need to access a Twitter API (application programming interface) to retrieve the information about your followers. In brief, an API is an intermediary application that allows applications to talk to each other. To access the Twitter APIs, you need a developer account for which you may apply at <https://developer.twitter.com/en>. There is a short form to fill in, and it takes less than a day to get an answer.

Below I rely heavily on the code shared by Joe Cristian through the Algoritma Technical Blog at <https://algotech.netlify.app/blog/social-network-analysis-in-r/>. Kuddos and credits to him. 

## Data retrieving

We load the `rtweet` package to work with Twitter from R. 
```{r}
#devtools::install_github("ropensci/rtweet")
library(rtweet)
```

We also load the `tidyverse` for data manipulation and visualisation.
```{r}
library(tidyverse)
theme_set(theme_light(base_size = 14))
```

First we need to get credentials. The usual `rtweet` sequence with `rtweet_app`, `auth_save` and `auth_as` is supposed to work (see [here](https://docs.ropensci.org/rtweet/articles/auth.html)), but the Twitter API kept failing (error 401) for me. Tried a few things, in vain. I will use the deprecated `create_token` function instead. You might need to change the defaults of your Twitter app from "Read only" to "Read, write and access direct messages". 
```{r echo = FALSE}
api_key <- "WrQ73W0AXDXMZyrEppAOXwYMa"
api_secret_key <- "3AWNze2UvARDk1QvF8COLNJtgcp0EqxR8NLKpCcnI8rV6AKZak"
# AAAAAAAAAAAAAAAAAAAAAAg%2BSAEAAAAA76yjMdORZe7XV4LyVpNgpZYZvO8%3DEQPK0YKcBv1i3nQUW5gttnLtnHpLA3P3grKetwqssGXw4zMX29
access_token <- "750892662224453632-zdorVXVejNxQM1DjQQaE9hdhL5A37HI"
access_token_secret <- "444GN6yJ7ufUHvwPxDrGlBQSBNLY0tN70ziBbwJphIt8H"
```
Enter my API keys and access tokens (not shown), then authenticate.
```{r}
token <- create_token(
  app = "sna-twitter-network-5k",
  consumer_key = api_key,
  consumer_secret = api_secret_key,
  access_token = access_token,
  access_secret = access_token_secret)
```

Get my Twitter info with my description, number of followers, number of likes, etc.
```{r}
og <- lookup_users("oaggimenez")
str(og, max = 2)
```

Now I obtain the id of my followers using `get_followers`.
```{r}
followers <- get_followers(user = "oaggimenez",
                           n = og$followers_count,
                           retryonratelimit = T)
```

From their id, I can get the same details I got on my account using `lookup_users`. This function is not vectorized, therefore I use a loop. Takes some time so I saved the results and load them.
```{r eval = FALSE}
details_followers <- NULL
for (i in 1:length(followers$user_id)){
  tmp <- try(lookup_users(followers$user_id[i], retryonratelimit = TRUE), silent = TRUE)
  if (length(tmp) == 1){
    next
  } else {
    tmp$listed_count <- NULL # get rid of this column which raised some format issues, we do not it anyway
    details_followers <- bind_rows(details_followers, tmp)
  }
}
save(details_followers, file = "details_followers.RData")
```

What info do we have?
```{r}
load("dat/details_followers.RData")
names(details_followers)
```

In more details.
```{r}
str(details_followers, max = 1)
```

## Data exploration and visualisation

Let's display the 100 bigger accounts that follow me. First thing I learned. It is humbling to be followed by influential individuals I admire like @MicrobiomDigest, @nathanpsmad, @FrancoisTaddei, @freakonometrics, @allison_horst, @HugePossum, @apreshill and scientific journals, institutions and societies. 
```{r}
details_followers %>% 
  arrange(-followers_count) %>%
  select(screen_name,
         followers_count, 
         friends_count, 
         favourites_count) %>% 
  head(n = 100)
```

Where do my followers come from? Second thing I learnt. Followers are folks from France (my home), USA, UK, Canada, Australia, and other European countries. I was happy to realize that I have followers from other parts of the world too, in India, South Africa, Brazil, Peru, Mexico, Nepal, Argentina, Kenya, Bangladesh, etc. and from Greece (my other home). 
```{r}
details_followers %>% 
  mutate(location = str_replace(location, ".*France.*", "France"),
         location = str_replace(location, ".*England", "United Kingdom"),
         location = str_replace(location, ".*United Kingdom", "United Kingdom"),
         location = str_replace(location, ".*London", "United Kingdom"),
         location = str_replace(location, ".*Scotland", "United Kingdom"),
         location = str_replace(location, ".*Montpellier", "France"),
         location = str_replace(location, ".*Canada, BC.*", "Canada"),
         location = str_replace(location, ".*New Caledonia", "France"),
         location = str_replace(location, ".*Marseille", "France"),
         location = str_replace(location, ".*Grenoble", "France"),
         location = str_replace(location, ".*Perú", "Peru"),
         location = str_replace(location, ".*Martinique", "France"),
         location = str_replace(location, ".*La Rochelle", "France"),
         location = str_replace(location, ".*Grenoble, Rhône-Alpes", "France"),
         location = str_replace(location, ".*Avignon", "France"),
         location = str_replace(location, ".*Paris", "France"),
         location = str_replace(location, ".*paris", "France"),
         location = str_replace(location, ".*france", "France"),
         location = str_replace(location, ".*Bordeaux", "France"),
         location = str_replace(location, ".*Mytilene, Lesvos, Greece", "Greece"),
         location = str_replace(location, ".*Mytiline, Greece", "Greece"),
         location = str_replace(location, ".*Germany.*", "Germany"),
         location = str_replace(location, ".*Vienna, Austria", "Austria"),
         location = str_replace(location, ".*Arusha, Tanzania", "Tanzania"),
         location = str_replace(location, ".*Athens", "Greece"),
         location = str_replace(location, ".*Bangor, ME", "United Kingdom"),
         location = str_replace(location, ".*Bangor, Wales", "United Kingdom"),
         location = str_replace(location, ".*Baton Rouge, LA", "United States of America"),
         location = str_replace(location, ".*Lisboa, Portugal", "Portugal"),
         location = str_replace(location, ".*Oslo", "Norway"),
         location = str_replace(location, ".*Islamic Republic of Iran", "Iran"),
         location = str_replace(location, ".*Madrid, España", "Spain"),
         location = str_replace(location, ".*Madrid", "Spain"),
         location = str_replace(location, ".*Madrid, Spain", "Spain"),
         location = str_replace(location, ".*Rome, Italy", "Italy"),
         location = str_replace(location, ".*Santiago, Chile", "Chile"),
         location = str_replace(location, ".*Belém, Brazil", "Brazil"),
         location = str_replace(location, ".*Belo Horizonte, Brazil", "Brazil"),
         location = str_replace(location, ".*Berlin", "Germany"),
         location = str_replace(location, ".*Mexico, ME", "Mexico"),
         location = str_replace(location, ".*Mexico City", "Mexico"),
         location = str_replace(location, ".*Santiago, Chile", "Chile"),
         location = str_replace(location, ".*Ontario", "Canada"),
         location = str_replace(location, ".*Aberdeen", "United Kingdom"),
         location = str_replace(location, ".*Cambridge", "United Kingdom"),
         location = str_replace(location, ".*Bologna, Emilia Romagna", "Italy"),
         location = str_replace(location, ".*Bogotá, D.C., Colombia", "Colombia"),
         location = str_replace(location, ".*Medellín, Colombia", "Colombia"),
         location = str_replace(location, ".*Bruxelles, Belgique", "Belgium"),
         location = str_replace(location, ".*Brasil", "Brazil"),
         location = str_replace(location, ".*Budweis, Czech Republic", "Czech Republic"),
         location = str_replace(location, ".*Calgary, Alberta", "Canada"),
         location = str_replace(location, ".*Canada, BC", "Canada"),
         location = str_replace(location, ".*Cardiff, Wales", "United Kingdom"),
         location = str_replace(location, ".*Brisbane", "Australia"),
         location = str_replace(location, ".*Sydney", "Australia"),
         location = str_replace(location, ".*Queensland", "Australia"),
         location = str_replace(location, ".*Australia", "Australia"),
         location = str_replace(location, ".*Germany", "Germany"),
         location = str_replace(location, ".*Vancouver", "Canada"),
         location = str_replace(location, ".*Ottawa, Ontario", "Canada"),
         location = str_replace(location, ".*Québec, Canada", "Canada"),
         location = str_replace(location, ".*Winnipeg, Manitoba", "Canada"),
         location = str_replace(location, ".*New South Wales", "Canada"),
         location = str_replace(location, ".*Victoria", "Canada"),
         location = str_replace(location, ".*British Columbia", "Canada"),
         location = str_replace(location, ".*Norway", "Norway"),
         location = str_replace(location, ".*Finland", "Finland"),
         location = str_replace(location, ".*South Africa", "South Africa"),
         location = str_replace(location, ".*Switzerland", "Switzerland"),
         location = str_replace(location, ".*CO", "United States of America"),
         location = str_replace(location, ".*OK", "United States of America"),
         location = str_replace(location, ".*KS", "United States of America"),
         location = str_replace(location, ".*MS", "United States of America"),
         location = str_replace(location, ".*CO", "United States of America"),
         location = str_replace(location, ".*CO", "United States of America"),
         location = str_replace(location, ".*CO", "United States of America"),
         location = str_replace(location, ".*CO", "United States of America"),
         location = str_replace(location, ".*WA", "United States of America"),
         location = str_replace(location, ".*MD", "United States of America"),
         location = str_replace(location, ".*Colorado", "United States of America"),
         location = str_replace(location, ".*Community of Valencia, Spain", "Spain"),
         location = str_replace(location, ".*Dunedin City, New Zealand", "New Zealand"),
         location = str_replace(location, ".*Rio Claro, Brasil", "Brazil"),
         location = str_replace(location, ".*Saskatoon, Saskatchewan", "Canada"),
         location = str_replace(location, ".*Sherbrooke, Québec", "Canada"),
         location = str_replace(location, ".*United Kingdom.*", "United Kingdom"),
         location = str_replace(location, ".*University of Oxford", "United Kingdom"),
         location = str_replace(location, ".*University of St Andrews", "United Kingdom"),
         location = str_replace(location, ".*ID", "United States of America"),
         location = str_replace(location, ".*NE", "United States of America"),
         location = str_replace(location, ".*United States of America, USA", "United States of America"),
         location = str_replace(location, ".*Spain, Spain", "Spain"),
         location = str_replace(location, ".*USA", "United States of America"),
         location = str_replace(location, ".*Wisconsin, USA", "United States of America"),
         location = str_replace(location, ".*Florida, USA", "United States of America"),
         location = str_replace(location, ".*Liege, Belgium", "Belgium"),
         location = str_replace(location, ".*Ghent, Belgium", "Belgium"),
         location = str_replace(location, ".*Pune, India", "India"),
         location = str_replace(location, ".*Hyderabad, India", "India"),
         location = str_replace(location, ".*Prague, Czech Republic", "Czech Republic"),
         location = str_replace(location, ".*Canada, BC, Canada", "Canada"),
         location = str_replace(location, ".*CA", "United States of America"),
         location = str_replace(location, ".*United States.*", "United States of America"),
         location = str_replace(location, ".*DC", "United States of America"),
         location = str_replace(location, ".*FL", "United States of America"),
         location = str_replace(location, ".*GA", "United States of America"),
         location = str_replace(location, ".*HI", "United States of America"),
         location = str_replace(location, ".*ME", "United States of America"),
         location = str_replace(location, ".*MA", "United States of America"),
         location = str_replace(location, ".*MI", "United States of America"),
         location = str_replace(location, ".*PA", "United States of America"),
         location = str_replace(location, ".*NC", "United States of America"),
         location = str_replace(location, ".*MO", "United States of America"),
         location = str_replace(location, ".*NY", "United States of America"),
         location = str_replace(location, ".*NH", "United States of America"),
         location = str_replace(location, ".*IL", "United States of America"),
         location = str_replace(location, ".*NM", "United States of America"),
         location = str_replace(location, ".*MT", "United States of America"),
         location = str_replace(location, ".*OR", "United States of America"),
         location = str_replace(location, ".*WY", "United States of America"),
         location = str_replace(location, ".*WI", "United States of America"),
         location = str_replace(location, ".*MN", "United States of America"),
         location = str_replace(location, ".*CT", "United States of America"),
         location = str_replace(location, ".*TX", "United States of America"),
         location = str_replace(location, ".*VA", "United States of America"),
         location = str_replace(location, ".*OH", "United States of America"),
         location = str_replace(location, ".*Massachusetts, USA", "United States of America"),
         location = str_replace(location, ".*California, USA", "United States of America"),
         location = str_replace(location, ".*Montréal, Québec", "Canada"),
         location = str_replace(location, ".*Edmonton, Alberta", "Canada"),
         location = str_replace(location, ".*Toronto, Ontario", "Canada"),
         location = str_replace(location, ".*Canada, Canada", "Canada"),
         location = str_replace(location, ".*Montreal", "Canada"),
         location = str_replace(location, ".*Lisbon, Portugal", "Portugal"),
         location = str_replace(location, ".*Coimbra, Portugal", "Portugal"),
         location = str_replace(location, ".*Cork, Ireland", "Ireland"),
         location = str_replace(location, ".*Dublin City, Ireland", "Ireland"),
         location = str_replace(location, ".*Barcelona, Spain", "Spain"),
         location = str_replace(location, ".*Barcelona", "Spain"),
         location = str_replace(location, ".*Leipzig", "Germany"),
         location = str_replace(location, ".*Seville, Spain", "Spain"),
         location = str_replace(location, ".*Seville, Spain", "Spain"),
         location = str_replace(location, ".*Buenos Aires, Argentina", "Argentina"),
         location = str_replace(location, ".*Rio de Janeiro, Brazil", "Brazil"),
         location = str_replace(location, ".*Canberra", "Australia"),
         location = str_remove(location, "Global"),
         location = str_remove(location, "Earth"),
         location = str_remove(location, "Worldwide"),
         location = str_remove(location, "Europe"),
         location = str_remove(location, "  "),
         location = str_replace(location, ".*Dhaka, Bangladesh", "Bangladesh"),
         location = str_replace(location, ".*Copenhagen, Denmark", "Denmark"),
         location = str_replace(location, ".*Amsterdam, The Netherlands", "The Netherlands"),
         location = str_replace(location, ".*Groningen, Nederland", "The Netherlands"),
         location = str_replace(location, ".*Wageningen, Nederland", "The Netherlands"),
         location = str_replace(location, ".*Aarhus, Denmark", "Denmark"),
         location = str_replace(location, ".*Antwerp, Belgium", "Belgium"),
         location = str_replace(location, ".*Aveiro, Portugal", "Portugal"),
         location = str_replace(location, ".*Australia, AUS", "Australia"),
         location = str_replace(location, ".*Australian National University", "Australia"),
         location = str_replace(location, ".*Auckland, New Zealand", "New Zealand"),
         location = str_replace(location, ".*Belfast, Northern Ireland", "United Kingdom"),
         location = str_replace(location, ".*Ireland", "United Kingdom"),
         location = str_replace(location, ".*Hobart, Tasmania", "Australia"),
         location = str_replace(location, ".*Dhaka, Bangladesh", "Bangladesh"),
         location = str_replace(location, ".*Nairobi, Kenya", "Kenya"),
         location = str_replace(location, ".*Dhaka, Bangladesh", "Bangladesh"),
         location = str_replace(location, ".*Berlin, Deutschland", "Germany"),
         location = str_replace(location, ".*Munich, Bavaria", "Germany"),
         location = str_replace(location, ".*Dehradun, India", "India"),
         location = str_replace(location, ".*Bengaluru, India", "India"),
         location = str_replace(location, ".*Berlin, Deutschland", "Germany"),
         location = str_replace(location, ".*Deutschland", "Germany"),
         location = str_replace(location, ".*Edinburgh", "United Kingdom"),
         location = str_replace(location, ".*Glasgow", "United Kingdom"),
         location = str_replace(location, ".*New York, USA", "United States of America"),
         location = str_replace(location, ".*Washington, USA", "United States of America"),
         location = str_replace(location, ".*California", "United States of America"),
         location = str_replace(location, ".*California", "United States of America"),
         location = str_replace(location, ".*Christchurch City, New Zealand", "New Zealand"),
         location = str_replace(location, ".*Harare, Zimbabwe", "Zimbabwe"),
         location = str_replace(location, ".*Islamabad, Pakistan", "Pakistan"),
         location = str_replace(location, ".*Kolkata, India", "India"),
         location = str_replace(location, ".*Lagos, Nigeria", "Nigeria"),
         location = str_replace(location, ".*Lima, Peru", "Peru"),
         location = str_replace(location, ".*Valparaíso, Chile", "Chile"),
         location = str_replace(location, ".*Uppsala, Sweden", "Sweden"),
         location = str_replace(location, ".*Uppsala, Sverige", "Sweden"),
         location = str_replace(location, ".*Stockholm, Sweden", "Sweden"),
         location = str_replace(location, ".*Stockholm", "Sweden"),
         location = str_replace(location, ".*Turin, Piedmont", "Italy"),
         location = str_replace(location, ".*University of Iceland", "Iceland"),
         location = str_replace(location, ".*University of Helsinki", "Finland"),
         location = str_replace(location, ".*Tucumán, Argentina", "Argentina"),
         location = str_replace(location, ".*The Hague, The Netherlands", "The Netherlands"),
         location = str_replace(location, ".*UK", "United Kingdom")
  ) %>%
  count(location, sort = TRUE) %>%
  #slice(-40) %>%
  head(n = 45) -> followers_by_country
followers_by_country
```

Let's have a look to the same info on a quick and dirty map. White countries do not appear in the 45 countries with most followers, or do not have any followers. 
```{r}
library(rworldmap)
library(classInt)
spdf <- joinCountryData2Map(followers_by_country, 
                            joinCode="NAME",
                            nameJoinColumn="location",
                            verbose=TRUE)
classInt <- classIntervals(spdf$n,
                           n=9, 
                           style = "jenks")
catMethod <- classInt[["brks"]]
library(RColorBrewer)
colourPalette <- brewer.pal(9,'RdPu')
mapParams <- mapCountryData(spdf,
                            nameColumnToPlot="n",
                            addLegend=FALSE,
                            catMethod = catMethod,
                            colourPalette=colourPalette,
                            mapTitle="Number of followers per country")
do.call(addMapLegend,
        c(mapParams,
          legendLabels="all",
          legendWidth=0.5,
          legendIntervals="data",
          legendMar = 2))
```

How many followers are verified?
```{r}
details_followers %>% 
  count(verified)
```

Who are the verified users? Scientific entities, but also some scientists. 
```{r}
details_followers %>%
  filter(verified==TRUE) %>%
  select(name, screen_name, location, followers_count) %>%
  arrange(-followers_count)
```

How old are my followers, where age is defined as the time elapsed since they created their Twitter account. Twitter was created in 2006. I created my account in July 2016 but started using it only two or three years ago I'd say. This dataviz is not too much informative I guess. 
```{r}
details_followers %>% 
  mutate(month = created_at %>% str_split(" ") %>% map_chr(2),
         day = created_at %>% str_split(" ") %>% map_chr(3),
         year = created_at %>% str_split(" ") %>% map_chr(6),
         created_at = paste0(month,"-",year),
         created_at = lubridate::my(created_at)) %>%
  group_by(created_at) %>%
  summarise(counts = n()) %>%
  ggplot(aes(x = created_at, y = counts)) +
  geom_line() +
  labs(x = "date of creation", y = "how many accounts were created this month-year")
```

What is the distribution of the number of followers and friends. On average, 400 followers and 400 friends. 
```{r}
foll <- details_followers %>%
  ggplot() + 
  aes(x = log(followers_count)) + 
  geom_histogram(fill="#69b3a2", color="#e9ecef", alpha=0.9) + 
  labs(x = "log number of followers", y = "counts", title = "followers")

friends <- details_followers %>%
  ggplot() + 
  aes(x = log(friends_count)) + 
  geom_histogram(fill="#69b3a2", color="#e9ecef", alpha=0.9) + 
  labs(x = "log number of friends", y = "", title = "friends")

library(patchwork)
foll + friends
```

## Retrieving social links

To investigate the social network made of my followers, we need to gather who they follow, and their followers. Unfortunately, the Twitter API imposes some time and quantity constraints in retrieving data.

Here, for the sake of illustration, I will focus on some accounts only. To do so, I consider accounts with between 1000 and 1500 followers, and more than 30000 tweets users have liked in their account's lifetime (which denotes some activity). The more followers we consider, the more time it would take to gather info on them: it is approx one minute per follower on average, so one hour for sixty followers, and more than eighty hours for all my followers! With the filters I'm using, I end up with 35 followers here, this is very few to say infer meaningful about a network, but it'll do the job for now. 
```{r}
some_followers <- details_followers %>% 
  filter((followers_count > 1000 & followers_count < 1500),
         favourites_count > 30000) # %>% nrow()
```

Create empty list and name it after their screen name.
```{r}
foler <- vector(mode = 'list', length = length(some_followers$screen_name))
names(foler) <- some_followers$screen_name
```

Get followers of these selected followers. Takes ages, so save and load later.
```{r eval = FALSE}
for (i in 1:length(some_followers$screen_name)) {
  message("Getting followers for user #", i, "/", nrow(some_followers))
  foler[[i]] <- get_followers(some_followers$screen_name[i], 
                              n = some_followers$followers_count[i], 
                              retryonratelimit = TRUE)
  if(i %% 5 == 0){
    message("sleep for 5 minutes")
    Sys.sleep(5*60)
    }
}
save(foler, file = "foler.RData")
```

Format the data. 
```{r}
load("dat/foler.RData")
folerx <- bind_rows(foler, .id = "screen_name")
active_fol_x <- some_followers %>% select(id_str, screen_name)
foler_join <- left_join(folerx, some_followers, by = "screen_name")
algo_follower <- foler_join %>% 
  select(id_str, screen_name) %>%
  setNames(c("follower", "active_user")) %>% 
  na.omit()
```

Get friends of my followers. Takes ages. Again I save and load later.
```{r eval = FALSE}
friend <- data.frame()
for (i in seq_along(some_followers$screen_name)) {
  message("Getting following for user #", i ,"/",nrow(some_followers))
  kk <- get_friends(some_followers$screen_name[i],
                    n = some_followers$friends_count[i],
                    retryonratelimit = TRUE)
  friend <- rbind(friend, kk)
  if(i %% 15 == 0){
    message("sleep for 15 minutes")
    Sys.sleep(15*60+1)
  }
}
save(friend, file = "friend.RData")
```

Format the data.
```{r}
load("dat/friend.RData")
all_friend <- friend %>% setNames(c("screen_name", "user_id"))
all_friendx <- left_join(all_friend, active_fol_x, by="screen_name")
algo_friend <- all_friendx %>% select(user_id, screen_name) %>%
  setNames(c("following","active_user"))
```

Now that we have all info on followers and friends, we're gonna build the network of people who follow each other. 
```{r}
un_active <- unique(algo_friend$active_user) %>% 
  data.frame(stringsAsFactors = F) %>%
  setNames("active_user")
algo_mutual <- data.frame()
for (i in seq_along(un_active$active_user)){
  aa <- algo_friend %>% 
    filter(active_user == un_active$active_user[i])
  bb <- aa %>% filter(aa$following %in% algo_follower$follower) %>%
    setNames(c("mutual","active_user"))
  
  algo_mutual <- rbind(algo_mutual,bb)
}
```

Instead of ids, we use screen names.
```{r}
detail_friend <- lookup_users(algo_mutual$mutual)
algo_mutual <- algo_mutual %>% 
  left_join(detail_friend, by = c("mutual" = "id_str")) %>% 
  na.omit() %>%
  select(mutual, active_user, screen_name)
algo_mutual
```

Add my account to the network. 
```{r}
un_active <- un_active %>% 
  mutate(mutual = rep("oaggimenez"))
un_active <- un_active[,c(2,1)]
un_active <- un_active %>% 
  setNames(c("active_user","screen_name"))
algo_mutual <- bind_rows(algo_mutual %>% select(-mutual), un_active)
```

For what follows, we will need packages to work with networks.
```{r}
library(igraph)
library(tidygraph)
library(ggraph)
```

Now we create the edges, nodes and build the network. 
```{r}
nodes <- data.frame(V = unique(c(algo_mutual$screen_name,algo_mutual$active_user)),
                    stringsAsFactors = F)

edges <- algo_mutual %>% 
  setNames(c("from","to"))
network_ego1 <- graph_from_data_frame(d = edges, vertices = nodes, directed = F) %>%
  as_tbl_graph()
```

## Network metrics

Create communities using `group_louvain()` algorithm, and calculate standard metrics using `tidygraph`. 
```{r}
set.seed(123)
network_ego1 <- network_ego1 %>% 
  activate(nodes) %>% 
  mutate(community = as.factor(group_louvain())) %>%
  mutate(degree_c = centrality_degree()) %>%
  mutate(betweenness_c = centrality_betweenness(directed = F,normalized = T)) %>%
  mutate(closeness_c = centrality_closeness(normalized = T)) %>%
  mutate(eigen = centrality_eigen(directed = F))
network_ego_df <- as.data.frame(network_ego1)
```

Identify key nodes with respect to network metrics.
```{r}
network_ego_df
```

Get users with highest values of each metric. 
```{r}
kp_ego <- data.frame(
  network_ego_df %>% arrange(-degree_c) %>% select(name),
  network_ego_df %>% arrange(-betweenness_c) %>% select(name),
  network_ego_df %>% arrange(-closeness_c) %>% select(name),
  network_ego_df %>% arrange(-eigen) %>% select(name)) %>% 
  setNames(c("degree","betweenness","closeness","eigen"))
kp_ego[-1,]
```

Robbie Emmet has the highest degree, betweenness, closeness centrality and eigenvector centrality. He has the most relations with the other nodes in the network. He also can spread information the further away and faster than anyone, and is also surrounded by important persons in the network. This is within the network we've just built. 

By the way, Robbie has just passed his dissertation defense, congrats! Follow him on Twitter at @robbie_emmet, he is awesome!

## Visualize Network

We visualize the network using clusters or communities (first three only).
```{r}
options(ggrepel.max.overlaps = Inf)
network_viz <- network_ego1 %>%
  filter(community %in% 1:3) %>%
  mutate(node_size = ifelse(degree_c >= 50,degree_c,0)) %>%
  mutate(node_label = ifelse(betweenness_c >= 0.01,name,NA))
plot_ego <- network_viz %>% 
  ggraph(layout = "stress") +
  geom_edge_fan(alpha = 0.05) +
  geom_node_point(aes(color = as.factor(community),size = node_size)) +
  geom_node_label(aes(label = node_label),nudge_y = 0.1,
                 show.legend = F, fontface = "bold", fill = "#ffffff66") +
  theme_graph() + 
  theme(legend.position = "none") +
  labs(title = "Top 3 communities")
plot_ego
```

Let's use an alternative dataviz to identify the nodes.
```{r}
network_viz %>% 
  mutate(community = group_spinglass()) %>%
  ggraph(layout = "nicely") +
  geom_edge_fan(alpha = 0.25) +
  geom_node_point(aes(color = factor(community)),size = 5, show.legend = F) +
  geom_node_text(aes(label = name),repel = T) +
  theme_graph() + theme(legend.position = "none") +
  labs(title = "Network with named nodes")
```

## Concluding words

Not sure what the communities are about. Due to limitation with the Twitter API I did not attempt to retrieve the info on all the 5,000 followers, but I used only 35 of them, which explains why there is not much to say about this network I guess. I might take some time to try and retrieve all the info (will take days), or not. I might also use some information I already have, like how followers describe themselves and do some topic modelling to identify common themes of interest. 

Here I have simply followed the steps that Joe Cristian illustrated so brillantly in his post at <https://algotech.netlify.app/blog/social-network-analysis-in-r/>. Make sure you read his post for more details on the code and theory. In particular, he illustrates how information spreads through the network. 

For more about social network analyses and Twitter, see this thread <https://twitter.com/Mehdi_Moussaid/status/1389174715990876160?s=20> and this video <https://www.youtube.com/watch?v=UX7YQ6m2r_o> in French, this is what inspired me to do the analysis. See also this <https://github.com/eleurent/twitter-graph> for a much better analysis than what I could do (with Python for data retrieving and manipulation, and [Gephi](https://gephi.org/) for network visualisation). 